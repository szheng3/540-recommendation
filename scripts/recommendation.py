# -*- coding: utf-8 -*-
"""recommendation-2-2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q4j51DSXFI1uBANQZFNuJim5Q1AjCXYC
"""

import pandas as pd
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import LabelEncoder
import os
import torch
from torch.utils.data import random_split
from torch.nn import MultiheadAttention

# read reviews.parquet file
reviews_df = pd.read_parquet('data/reviews.parquet')

# read recipes.parquet file
recipes_df = pd.read_parquet('data/recipes.parquet')

# print the first 5 rows of reviews_df
reviews_df.head()

recipes_df.head()

data = pd.merge(reviews_df, recipes_df, on='RecipeId')

# print the merged dataframe
data.head()

df = data[["RecipeId", "AuthorId_y", "Rating", "Calories", "ReviewCount"]]

# remove duplicates without considering the first column
df = df[~df.iloc[:, 1:].duplicated(keep='first', subset=df.columns[1:])]

# reset the index and drop the original index column
df = df.reset_index(drop=True)

# print the updated dataframe
df.head()

class RecipeDataset(Dataset):
    def __init__(self, data):
        self.user_encoder = LabelEncoder()
        self.item_encoder = LabelEncoder()
        self.recipe_ids = self.item_encoder.fit_transform(data["RecipeId"].values)
        self.author_ids = self.user_encoder.fit_transform(data["AuthorId_y"].values)

        self.ratings = data["Rating"].astype(float).values
        self.calories = data["Calories"].astype(float).values
        self.review_counts = data["ReviewCount"].astype(float).values

    def __len__(self):
        return len(self.recipe_ids)

    def __getitem__(self, idx):
        recipe_id = self.recipe_ids[idx]
        author_id = self.author_ids[idx]
        rating = self.ratings[idx]
        calories = self.calories[idx]
        review_count = self.review_counts[idx]
        return (recipe_id, author_id, calories, review_count), rating

import torch.nn.functional as F

data = RecipeDataset(data)

# Split the dataset into training and validation sets
train_size = int(0.8 * len(data))
val_size = len(data) - train_size
train_dataset, val_dataset = random_split(data, [train_size, val_size])

# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
valid_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
saved_models_dir = 'saved_models'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class RecipeModel(torch.nn.Module):
    def __init__(self, num_recipes, num_authors, max_calories, max_review_counts, embedding_dim=16, num_heads=4):
        super().__init__()
        self.recipe_embedding = torch.nn.Embedding(num_recipes, embedding_dim)
        self.author_embedding = torch.nn.Embedding(num_authors, embedding_dim)
        self.calorie_embedding = torch.nn.Embedding(max_calories, embedding_dim)
        self.review_count_embedding = torch.nn.Embedding(max_review_counts, embedding_dim)

        # self.embedding_transform = torch.nn.Linear(embedding_dim * 4, embedding_dim)
        self.attention = MultiheadAttention(embed_dim=embedding_dim*4, num_heads=num_heads)
        # self.fc1 = torch.nn.Linear(embedding_dim * 4, 16)

        self.fc1 = torch.nn.Linear(embedding_dim * 4, 16)
        self.fc2 = torch.nn.Linear(16, 1)
        self.activation = torch.nn.Sigmoid()

    def forward(self, recipe_id, author_id, calories, review_counts):
        recipe_embedded = self.recipe_embedding(recipe_id)
        author_embedded = self.author_embedding(author_id)
        calorie_embedded = self.calorie_embedding(calories.long())
        review_count_embedded = self.review_count_embedding(review_counts.long())

        x = torch.cat([recipe_embedded, author_embedded, calorie_embedded, review_count_embedded], dim=-1)
        # x = self.embedding_transform(x)
        
        x = x.unsqueeze(1)

        attn_output, _ = self.attention(x, x, x)
        
        x = attn_output.squeeze(1)

        # x = torch.mean(attn_output, dim=1)  # Compute the mean along the sequence_length dimension

        x = self.fc1(x)
        # x = F.relu(x)
        x = self.fc2(x)
        x = self.activation(x)

        return (x * 5.0).view(-1)




# Calculate the maximum values for Calories and ReviewCount
max_calories = int(df["Calories"].max())
max_review_count = int(df["ReviewCount"].max())

model = RecipeModel(num_recipes=len(data.item_encoder.classes_) + 1, 
                    num_authors=len(data.user_encoder.classes_) + 1,
                    max_calories=max_calories+1,
                    max_review_counts=max_review_count+1)

# Initialize the best validation loss to a large value
best_valid_loss = float('inf')

# Create a directory for the saved models if it doesn't exist
os.makedirs(saved_models_dir, exist_ok=True)
# RecipeModel(num_recipes=data.loc[:,'RecipeId'].max()+1, num_authors=data.loc[:,"AuthorId_y"].max()+1)
model = model.to(device) # Send model to GPU if available

criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

num_epochs = 10
for epoch in range(num_epochs):
    model.train()
    train_loss = 0
    for batch, targets in train_loader:
        optimizer.zero_grad()
        batch = [b.to(device) for b in batch]
        targets = targets.float().to(device)
        preds = model(*batch)

        loss = criterion(preds, targets)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * batch[0].shape[0]
    train_loss /= len(train_dataset)

    model.eval()
    valid_loss = 0
    with torch.no_grad():
        for batch, targets in valid_loader:
            batch = [b.to(device) for b in batch]
            targets = targets.float().to(device)
            preds = model(*batch)
            loss = criterion(preds, targets)
            valid_loss += loss.item() * batch[0].shape[0]
        valid_loss /= len(val_dataset)

    print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}")

    # Check if the current validation loss is lower than the best validation loss
    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        print(f"Validation loss improved. Saving the model to {saved_models_dir}/best_model.pt")
        torch.save(model.state_dict(), f"{saved_models_dir}/best_model.pt")

model.load_state_dict(torch.load(f"{saved_models_dir}/best_model.pt"))
model = model.to(device) # Send model to GPU if available

author_id = 1545
recipe_ids = df["RecipeId"].unique()[:10000]
# recipe_ids = df["RecipeId"].unique()


user_has_ratings = author_id in df["AuthorId_y"].values

if user_has_ratings:
    user_rated_recipe_ids = df[df["AuthorId_y"] == author_id]["RecipeId"].unique()
else:
    user_rated_recipe_ids = []

# Create a recommendation dataset
recommendation_data = []
for recipe_id in recipe_ids:
    if not user_has_ratings or (user_has_ratings and recipe_id not in user_rated_recipe_ids):
        recipe_id_transformed = data.item_encoder.transform([recipe_id])[0]
        recipe_data = df[df["RecipeId"] == recipe_id].iloc[0]
        recommendation_data.append((recipe_id_transformed, author_id, recipe_data["Calories"], recipe_data["ReviewCount"]))

recommendation_dataset = [(torch.tensor(a).to(device), torch.tensor(b).to(device), torch.tensor(c).to(device), torch.tensor(d).to(device)) for a, b, c, d in recommendation_data]
recommendation_loader = DataLoader(recommendation_dataset, batch_size=batch_size, shuffle=False)
# Model evaluation
model.eval()
with torch.no_grad():
    ratings = []
    for inputs in recommendation_loader:
        rating = model(*inputs)
        ratings.extend(rating.detach().cpu().numpy())

top_recipe_ids = [recipe_ids[i] for i in sorted(range(len(ratings)), key=lambda i: ratings[i], reverse=True)[:10]]
print(top_recipe_ids)


